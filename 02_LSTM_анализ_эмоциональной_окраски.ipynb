{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как обычно импортируем все, что нужно.\n",
    "# Из необычного тут только sklearn - полезная вещь,\n",
    "# на нее тоже есть хорошая документация.\n",
    "# Нам конкретно оттуда надо train_test_split.\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# Очень полезная вещь для работы с текстовыми вещами.\n",
    "# Детальное описание на ее сайте.\n",
    "# При первом запуске надо раскоментить следующую строчку.\n",
    "# Высплывет окошко и можно выбрать, что скачать.\n",
    "# Можно не мучаться и скачать все.\n",
    "# Для этого можно указать nltk.download(\"all\").\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "# Загрузка файла и исследоватеслький кусок кода.\n",
    "# Хочется просто понять, сколько уникальных слов встречается\n",
    "# в корпусе текстов и сколько в каждом предложениии.\n",
    "# Что происходит: объявляем переменные, создают пустой счетчик,\n",
    "# открываем файл и проходимся по каждой строчке файла.\n",
    "# Для каждой строчки делаем две подстроки - метка и само предложение\n",
    "# (они разделены в файле с помощью табуляции \"\\t\").\n",
    "# Далее делаем nltk.word_tokenize - классная вещь, игнорирует части речи,\n",
    "# склонения и прочие параметры, которые для нашего демонстрационного\n",
    "# примера не нужны.\n",
    "# Ну а затем собствено обновляем наши переменные.\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"D:/Data/train.txt\", 'r', encoding = 'utf-8')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")    \n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    num_recs += 1\n",
    "ftrain.close()\n",
    "\n",
    "# Собственно вывыодим числа и смотрим на них.\n",
    "print(maxlen)\n",
    "print(len(word_freqs))\n",
    "\n",
    "# Наши две ключевые константы.\n",
    "# Мы задаем фиксированный размер словаря, а все остальные слова\n",
    "# считаем несловарными и заменим впоследствии их фиктивноым словом UNK.\n",
    "# Это позволит на этапе предсказания обрабатывать ранее не встречавшиеся\n",
    "# слова как несловарные.\n",
    "# Вторая константа задает фиксированную длину предложения и более\n",
    "# короткие предложения будут дополнены нулями, а более длинные обрезаны.\n",
    "# Формально можно это не делать, ибо РНС позволяет обрабатывать\n",
    "# последовательности переменной длины, но мы для простоты сети\n",
    "# будем обрабатывать предложения фиксированной длины.\n",
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "\n",
    "# Делаем таблицы соответствия.\n",
    "# Входными данными для РНС является строка индексов слов, причем\n",
    "# слова упорядочены по убыванию частоыт встречаемости в обучающем наборе\n",
    "# (это важно, когда-нибудь покажу почему).\n",
    "# Таблицы соответствия позволяют находить индекс по слову\n",
    "# и слово по индексу (включая фиктивные слова PAD и UNK).\n",
    "# 1 is UNK, 0 is PAD\n",
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in \n",
    "                enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "# Здесь мы преобразуем входные предложения в последотельности\n",
    "# индексов слов, дополняя (или обрезая) их до нужной длины.\n",
    "# Выходные вещи (метки) специальным образом обрабатывать не надо.\n",
    "X = np.empty((num_recs, ), dtype = list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(\"D:/Data/train.txt\", 'r', encoding = 'utf-8')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "# Эта функция собственно делает предложения нужной длины.\n",
    "X = sequence.pad_sequences(X, maxlen = MAX_SENTENCE_LENGTH)\n",
    "\n",
    "# Этой функцией делаем разбивку всего набора данных на обучающий и\n",
    "# тестовый в пропорции 80:20.\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,\n",
    "                                                test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Тупо вывпедем, чтобы глянуть.\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)\n",
    "\n",
    "# Констатны для сети, их назначение реально должно быть понятно.\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Строим саму модель.\n",
    "# Входными данными являеться последостность индексов слов.\n",
    "# Длина последовательности равна MAX_SENTENCE_LENGTH.\n",
    "# Первому измерению тензора присваивается значение None, показывающее,\n",
    "# что размер пакета (чило записей, загружаемых в сеть за один раз)\n",
    "# в момент определения сети неизвестен. он будет задан на этапе\n",
    "# выполнения с помощью параметра batch_size.\n",
    "# Таким образом в предположении, что размер пакет пока неизвестен,\n",
    "# входной тензор имеет форму (None, MAX_SENTENCE_LENGTH, 1).\n",
    "# Такие тензоры подаются на вход слоя погружения размера EMBEDDING_SIZE,\n",
    "# веса которого инициализированы небольшими случайными значениями и\n",
    "# подлежат обучению. Этот слой преобразует выходной тензор к форме\n",
    "# (None, MAX_SENTENCE_LENGTH, EMBEDDING_SIZE). Выход слоя погружения\n",
    "# загружается в LSTM с длиной последовательности MAX_SENTENCE_LENGTH\n",
    "# и размером выходного слоя HIDDEN_LAYER_SIZE. На выходе LSTM получается\n",
    "# тензор формы (None, HIDDEN_LAYER_SIZE, MAX_SENTENCE_LENGTH).\n",
    "# По умолчанию LSTM выводи единственный тензор формы (None, HIDDEN_LAYER_SIZE)\n",
    "# в качестве результирующей последовательности. Он подается на вход плотного\n",
    "# слоя с размером выхода 1 и сигмоидной функцией активации.\n",
    "# Ух... Тяжело, наверное. \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, \n",
    "                    input_length = MAX_SENTENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# Компилируем модель, ничего интересного и сложного.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Обучаем модель.\n",
    "history = model.fit(Xtrain, ytrain, batch_size = BATCH_SIZE, \n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    validation_data = (Xtest, ytest))\n",
    "\n",
    "# Тестируем модель.\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size = BATCH_SIZE)\n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
    "\n",
    "# Играемся с моделью.\n",
    "# Выбираем случайные предложения из тестового набора и печатаем\n",
    "# предсказание РНС, реальную метку и само предложение.\n",
    "for i in range(50):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,40)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
